import os

import numpy as np
import torch.nn.functional as F
import pandas as pd
import torch
from collections import defaultdict, Counter
from sklearn.metrics import confusion_matrix, classification_report
from matplotlib import pyplot as plt
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report
from tqdm import tqdm
from sklearn.metrics import confusion_matrix
import seaborn as sns
from models.MLP import FiveClassClassifier, FiveClassClassifier_tcga,MLPClassifier


def load_data(csv_file, feature_folder):
    """åŠ è½½ CSV å¹¶å°†æ•°æ®è½¬æ¢ä¸ºå­—å…¸æ ¼å¼."""
    data = pd.read_csv(csv_file)
    wsi_features = defaultdict(list)
    labels = {}

    # å®šä¹‰æ ‡ç­¾æ˜ å°„
    label_mapping = {'J': 1, 'N': 2, 'T': 3, 'Z': 4}
    # label_mapping = {'HGSC': 1, 'LGSC': 2, 'CC': 3, 'MC': 4, 'EC':5}
    for index, row in data.iterrows():
        slide_id = row['slide_id']
        label = row['label']

        # å°†æ ‡ç­¾è½¬æ¢ä¸ºæ•°å­—
        numeric_label = label_mapping.get(label)

        # æ„é€ ç‰¹å¾æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        feature_path = os.path.join(feature_folder, f"{slide_id}.pt")

        # åŠ è½½å¯¹åº”çš„ patch ç‰¹å¾å‘é‡
        if os.path.exists(feature_path):
            feature = torch.load(feature_path)
            wsi_features[slide_id].append(feature)
            labels[slide_id] = numeric_label
        else:
            print(f"ç‰¹å¾æ–‡ä»¶ {feature_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡è¯¥æ¡ç›®ã€‚")

    return wsi_features, labels


def classify_and_aggregate(model, wsi_features, device, num_classes, batch_size=4096):
    """
    å¯¹WSIè¿›è¡Œåˆ†ç±»å’Œèšåˆã€‚

    1. ç¡¬é¢„æµ‹ (Acc/F1): é€šè¿‡å¤šæ•°æŠ•ç¥¨å¾—åˆ°ã€‚
    2. è½¯æ¦‚ç‡ (AUC): é€šè¿‡å¹³å‡éèƒŒæ™¯ Patch çš„ Logitsï¼Œå†åº”ç”¨ Softmax å¾—åˆ°ã€‚
    """
    model.eval()
    wsi_hard_predictions = {}  # å­˜å‚¨å¤šæ•°æŠ•ç¥¨ç»“æœ (ç”¨äº Acc/F1/CM)
    wsi_soft_probabilities = {}  # å­˜å‚¨ Logits å¹³å‡å Softmax çš„æ¦‚ç‡ (ç”¨äº AUC)

    with torch.no_grad():
        for wsi_id, features in tqdm(wsi_features.items(), desc='Processing WSIs'):

            all_patch_preds_list = []  # å­˜å‚¨æ‰€æœ‰ patch çš„ç¡¬é¢„æµ‹ (ç”¨äºæŠ•ç¥¨)
            all_patch_logits_list = []  # å­˜å‚¨æ‰€æœ‰ patch çš„ Logits (ç”¨äºå¹³å‡)
            features = features[0]
            # --- é˜¶æ®µä¸€ï¼šæ‰¹é‡å¤„ç† Patch å¹¶æ”¶é›†æ•°æ® ---
            for i in range(0, len(features), batch_size):
                batch_features = features[i:i + batch_size].to(device)

                # æ¨¡å‹è¾“å‡º Logits (å½¢çŠ¶: [batch_size, num_classes])
                outputs = model(batch_features)

                # 1. æ”¶é›† Logitsï¼šç”¨äº WSI æ¦‚ç‡çš„å¹³å‡æ± åŒ–
                all_patch_logits_list.append(outputs.cpu())

                # 2. è®¡ç®— Patch æ ‡ç­¾ï¼šç”¨äº WSI å¤šæ•°æŠ•ç¥¨
                batch_preds = torch.argmax(outputs, dim=1)
                all_patch_preds_list.extend(batch_preds.cpu().tolist())

            # --- é˜¶æ®µäºŒï¼šWSI çº§åˆ«èšåˆ ---

            # è¿‡æ»¤æ‰èƒŒæ™¯ç±»åˆ« (class 0) - ä¸ä½ çš„åŸå§‹é€»è¾‘ä¿æŒä¸€è‡´
            non_background_indices = [
                idx for idx, pred in enumerate(all_patch_preds_list) if pred != 0
            ]

            non_background_preds = [
                pred for pred in all_patch_preds_list if pred != 0
            ]

            # 1. è®¡ç®— WSI çš„ç¡¬é¢„æµ‹ (å¤šæ•°æŠ•ç¥¨)
            if non_background_preds:
                prediction_counts = Counter(non_background_preds)
                final_hard_prediction = prediction_counts.most_common(1)[0][0]
            else:
                final_hard_prediction = 0

            wsi_hard_predictions[wsi_id] = final_hard_prediction

            # 2. è®¡ç®— WSI çš„è½¯æ¦‚ç‡ (Logits å¹³å‡å Softmax)
            if not all_patch_logits_list:
                # WSI ä¸ºç©ºï¼Œä½¿ç”¨å‡åŒ€æ¦‚ç‡
                wsi_avg_prob_np = np.ones(num_classes) / num_classes
            else:
                # åˆå¹¶æ‰€æœ‰ batch çš„ Logits tensor
                all_patch_logits_tensor = torch.cat(all_patch_logits_list, dim=0)

                if non_background_indices:
                    # å…³é”®ï¼šåªå¯¹éèƒŒæ™¯ Patch çš„ Logits è¿›è¡Œå¹³å‡
                    non_bg_patch_logits = all_patch_logits_tensor[non_background_indices]
                    wsi_avg_logits = torch.mean(non_bg_patch_logits, dim=0)  # [num_classes]
                else:
                    # å¦‚æœå…¨éƒ¨æ˜¯èƒŒæ™¯ (é0çš„ç¡¬æ ‡ç­¾ä¸ºç©º)ï¼Œåˆ™å¹³å‡æ‰€æœ‰ Patch çš„ Logits
                    wsi_avg_logits = torch.mean(all_patch_logits_tensor, dim=0)

                    # å¯¹å¹³å‡ Logits åº”ç”¨ Softmax å¾—åˆ°æ¦‚ç‡
                wsi_avg_prob = F.softmax(wsi_avg_logits.unsqueeze(0), dim=1).squeeze(0)
                wsi_avg_prob_np = wsi_avg_prob.numpy()

            wsi_soft_probabilities[wsi_id] = wsi_avg_prob_np

    return wsi_hard_predictions, wsi_soft_probabilities


def classify_and_vote(model, wsi_features, device, num_classes, batch_size=32):
    model.eval()
    wsi_final_predictions = {}
    wsi_final_probabilities = {}
    with torch.no_grad():
        for wsi_id, features in tqdm(wsi_features.items(), desc='Processing WSIs'):
            predictions = []
            for i in range(0, len(features), batch_size):
                batch_features = torch.stack(features[i:i + batch_size]).to(device)
                outputs = model(batch_features)
                outputs = outputs.squeeze(0)
                predicted = torch.argmax(outputs, dim=1)
                predictions.extend(predicted.cpu().tolist())

            predictions = [pred for pred in predictions if pred != 0]

            if predictions:
                prediction_counts = Counter(predictions)
                final_prediction = prediction_counts.most_common(1)[0][0]
            else:
                final_prediction = 0
            # åˆå§‹åŒ–æ¯ä¸ªç±»åˆ«è®¡æ•°ä¸º0
            counts = torch.zeros(num_classes, dtype=torch.float)

            # æ›´æ–°å„ç±»åˆ«è®¡æ•°
            for cls, count in prediction_counts.items():
                counts[cls] = count

            probabilities = F.softmax(counts, dim=0)

            wsi_final_predictions[wsi_id] = final_prediction
            wsi_final_probabilities[wsi_id] = probabilities.numpy()

    return wsi_final_predictions, wsi_final_probabilities




def calculate_metrics(true_labels, predicted_labels):
    print(classification_report(true_labels, predicted_labels,digits=4))
    overall_accuracy = accuracy_score(true_labels, predicted_labels)
    print(f"æ€»ä½“å‡†ç¡®ç‡: {overall_accuracy}")
    f1 = f1_score(true_labels, predicted_labels, average='weighted')
    print(f"F1 åˆ†æ•°: {f1}")

    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(true_labels, predicted_labels)
    print("æ··æ·†çŸ©é˜µ:")
    print(cm)
    plot_confusion_matrix(cm, class_names=['1', '2', '3', '4'])




def plot_confusion_matrix(cm, class_names):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.title('Confusion Matrix')
    plt.savefig('/home/idao/Zyf/models/oc_subtype_classficter/Autoencoder/fold1/æ¶ˆè')
    plt.show()

if __name__ == "__main__":
    csv_file = "/home/idao/Zyf/models/oc_subtype_classficter/Dataset/flod_1/test_data.csv"
    pt_file = '/home/idao/Zyf/data/oc_features/FEATURES_DIRECTORY/pt_files'
    # åŠ è½½æ•°æ®
    wsi_features, labels = load_data(csv_file,pt_file)
    num_classes = 5
    # è¿›è¡Œåˆ†ç±»å’Œå¤šæ•°æŠ•ç¥¨
    model = FiveClassClassifier(input_dim=1024, num_classes=5)
    # model = MLPClassifier(input_size=1024,hidden_size=128,num_classes=5)
    model.load_state_dict(torch.load("/home/idao/Zyf/models/oc_subtype_classficter/multi_view/fiveClassifier0.9433.pth"))
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # hard_results: å¤šæ•°æŠ•ç¥¨çš„ç¡¬æ ‡ç­¾ (ç”¨äº Acc, F1, CM)
    # soft_probabilities: Logits å¹³å‡åçš„è½¯æ¦‚ç‡ (ç”¨äº AUC)
    hard_results, soft_probabilities = classify_and_aggregate(model, wsi_features, device, num_classes)

    # --- 1. å‡†å¤‡ç”¨äº Acc/F1/CM çš„æ•°æ® (ä½¿ç”¨ç¡¬æ ‡ç­¾) ---
    # ç¡®ä¿ true_labels å’Œ predicted_labels é¡ºåºä¸€è‡´
    true_labels_list = [label for _, label in labels.items()]
    predicted_labels_list = [prediction for _, prediction in hard_results.items()]

    # è®¡ç®— Acc/F1/CM
    calculate_metrics(true_labels_list, predicted_labels_list)

    # --- 2. å‡†å¤‡ç”¨äº AUC çš„æ•°æ® (ä½¿ç”¨è½¯æ¦‚ç‡) ---
    # ç¡®ä¿ true_labels å’Œ probabilities çš„é¡ºåºä¸€è‡´
    slide_ids_ordered = list(labels.keys())

    # çœŸå®çš„æ ‡ç­¾ (y_true)ï¼Œå½¢çŠ¶: (n_samples,)ï¼Œå€¼æ˜¯ 1, 2, 3, 4
    true_labels_array = np.array([labels[slide_id] for slide_id in slide_ids_ordered])

    # é¢„æµ‹çš„æ¦‚ç‡ (y_pred_proba)ï¼Œå½¢çŠ¶: (n_samples, 5)ï¼ŒåŒ…å«ç±»åˆ« 0 åˆ° 4 çš„æ¦‚ç‡
    all_probabilities_for_auc = np.array([soft_probabilities[slide_id] for slide_id in slide_ids_ordered])

    # 1. ğŸ’¥ å…³é”®ä¿®æ”¹ï¼šåˆ‡ç‰‡æ¦‚ç‡æ•°ç»„ï¼Œç§»é™¤ç¬¬ 0 åˆ— (èƒŒæ™¯ç±»)
    # y_score_for_auc çš„å½¢çŠ¶æ˜¯ (n_samples, 4)ï¼Œå¯¹åº”äºæ ‡ç­¾ 1, 2, 3, 4
    y_score_raw = all_probabilities_for_auc[:, 1:]

    # 2. ğŸ’¥ å…³é”®ä¿®æ”¹ï¼šé‡æ–°è§„èŒƒåŒ– (Normalization)
    # è®¡ç®—æ¯è¡Œï¼ˆå³æ¯ä¸ª WSIï¼‰çš„éèƒŒæ™¯æ¦‚ç‡å’Œ
    row_sums = y_score_raw.sum(axis=1, keepdims=True)

    # é¿å…é™¤ä»¥é›¶ (å¦‚æœæŸä¸ª WSI çš„æ‰€æœ‰éèƒŒæ™¯æ¦‚ç‡éƒ½ä¸º 0ï¼Œåˆ™è®¾ä¸ºå‡åŒ€åˆ†å¸ƒæˆ–ä¿ç•™åŸæ ·)
    # è¿™é‡Œå‡è®¾æ‰€æœ‰ WSI è‡³å°‘æœ‰ä¸€ä¸ªéé›¶çš„æ¦‚ç‡å’Œ
    row_sums[row_sums == 0] = 1.0  # å¦‚æœå’Œä¸º0ï¼Œè®¾ä¸º1ï¼Œé¿å…NaNï¼Œè®©å…¶ä¿ç•™å‡åŒ€åˆ†å¸ƒ

    # è§„èŒƒåŒ–ï¼šå°†æ¯è¡Œçš„æ¦‚ç‡é™¤ä»¥è¯¥è¡Œçš„å’Œ
    y_score_for_auc = y_score_raw / row_sums

    # éªŒè¯ï¼šæ£€æŸ¥ y_score_for_auc.sum(axis=1) æ˜¯å¦æ¥è¿‘ 1.0 (å¯é€‰)
    # print("è§„èŒƒåŒ–åæ¯è¡Œä¹‹å’Œï¼ˆåº”æ¥è¿‘ 1.0ï¼‰:", y_score_for_auc.sum(axis=1))

    # å®šä¹‰æˆ‘ä»¬å…³å¿ƒçš„ç±»åˆ« (å³ label_mapping ä¸­çš„éèƒŒæ™¯ç±»åˆ«)
    class_labels_of_interest = np.unique(true_labels_array)
    class_labels_of_interest = class_labels_of_interest[class_labels_of_interest != 0]

    print("\n--- è®¡ç®—å®å¹³å‡ AUC (OvR) ---")
    try:
        # ç°åœ¨ y_score_for_auc æ˜¯è§„èŒƒåŒ–çš„ 4 åˆ—æ¦‚ç‡ï¼Œä¸ 4 ä¸ªæ ‡ç­¾åŒ¹é…
        auc_score_macro = roc_auc_score(
            true_labels_array,  # åŸå§‹æ•´æ•°æ ‡ç­¾ [1, 2, 4, 3, ...]
            y_score_for_auc,  # è§„èŒƒåŒ–çš„ 4 åˆ—æ¦‚ç‡ (P'(1) åˆ° P'(4))
            multi_class='ovr',  # One-vs-Rest ç­–ç•¥
            average='macro',  # è®¡ç®—å®å¹³å‡
            # ç§»é™¤ labels å‚æ•°ï¼Œå› ä¸ºåˆ—æ•°å’Œæ ‡ç­¾æ•°å·²åŒ¹é…
        )
        print(f"å®å¹³å‡ AUC (Macro-Average AUC, OvR): {auc_score_macro:.4f}")

    except ValueError as e:
        print(f"è®¡ç®— AUC æ—¶å‡ºé”™: {e}")
        print("æç¤º: ç¡®ä¿æ¯ä¸ªè¢«è¯„ä¼°çš„ç±»åˆ« (1, 2, 3, 4) åœ¨æµ‹è¯•é›†ä¸­è‡³å°‘åŒ…å«ä¸€ä¸ªæ­£æ ·æœ¬å’Œä¸€ä¸ªè´Ÿæ ·æœ¬ã€‚")




# å¼•å…¥ç»˜åˆ¶ ROC æ›²çº¿æ‰€éœ€çš„å·¥å…·
    from sklearn.preprocessing import LabelBinarizer  # ç”¨äº One-Hot ç¼–ç çœŸå®æ ‡ç­¾
    from sklearn.metrics import roc_curve, auc
    import matplotlib.pyplot as plt

    # ç±»åˆ«æ ‡ç­¾åç§°ï¼ˆç”¨äºå›¾ä¾‹ï¼ŒåŸºäºä½ çš„ label_mappingï¼‰
    # ç¡®ä¿ class_names åªåŒ…å«ä½ è¦è¯„ä¼°çš„ç±»åˆ« (1, 2, 3, 4, 5)
    # ä½ çš„ label_mapping æ˜¯ {'HGSC': 1, 'LGSC': 2, 'CC': 3, 'MC': 4, 'EC':5 }
    # å‡è®¾ç±»åˆ« 0 æ˜¯èƒŒæ™¯ï¼Œä¸éœ€è¦ç»˜åˆ¶ã€‚
    # æ£€æŸ¥ true_labels_array ä¸­çš„æœ€å¤§å€¼ï¼Œç¡®ä¿å®ƒåœ¨ class_names èŒƒå›´å†…ã€‚

    # æ ¹æ®ä½ çš„ num_classes=6 å’Œæ ‡ç­¾ 1-5ï¼Œæˆ‘ä»¬å…³æ³¨ 5 ä¸ªç±»åˆ«ã€‚
    class_names = {1: 'J',  2: 'N', 3: 'T', 4: 'Z'}

    # --- 1. å‡†å¤‡ OvR ç»˜å›¾æ•°æ® ---
    # a. å°†çœŸå®çš„æ•´æ•°æ ‡ç­¾è½¬æ¢ä¸º One-Hot ç¼–ç 
    lb = LabelBinarizer()
    # é€‚åº”äºæ‰€æœ‰å¯èƒ½çš„æ ‡ç­¾ (1, 2, 3, 4)
    lb.fit(np.array(list(class_names.keys())))
    binarized_true_labels = lb.transform(true_labels_array)  # å½¢çŠ¶: (n_samples, 4)

    # b. é¢„æµ‹æ¦‚ç‡ y_score_for_auc å½¢çŠ¶ä¸º (n_samples, 5)ï¼Œå¯¹åº”æ ‡ç­¾ 1 åˆ° 4
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä½ çš„ num_classes=6 æ„å‘³ç€ç±»åˆ«æ˜¯ 0 åˆ° 5ï¼Œ
    # ä½ çš„ y_score_for_auc åº”è¯¥æ˜¯ 5 åˆ— (1:5)ã€‚æˆ‘ä»¬æ²¿ç”¨ä¹‹å‰çš„åˆ‡ç‰‡é€»è¾‘ã€‚
    # **é‡è¦ï¼šé‡æ–°ç¡®è®¤ y_score_for_auc çš„å½¢çŠ¶ï¼šå®ƒåº”è¯¥æœ‰ 5 åˆ— (å¯¹åº” 1, 2, 3, 4, 5)**
    # ä½ çš„ label_mapping æœ‰ 5 ä¸ªç±»åˆ« (1-5)ï¼Œnum_classes=6 (0-5)ã€‚
    # å› æ­¤ y_score_raw = all_probabilities_for_auc[:, 1:] åº”è¯¥æ˜¯ 5 åˆ—ã€‚

    # ä¸ºæ¯ä¸ªç±»åˆ«è®¡ç®— ROC æ›²çº¿æ•°æ®
    fpr = dict()
    tpr = dict()
    # roc_auc = dict() # å¯ä»¥åœ¨è¿™é‡Œè®¡ç®—ï¼Œä½†ä½ å·²ç»è®¡ç®—äº†å®å¹³å‡

    # è¿­ä»£ç±»åˆ« 1 åˆ° 5
    evaluated_class_ids = list(class_names.keys())

    # c. ä¿®æ­£ y_score_for_auc çš„åˆ—æ•°ä»¥åŒ¹é…ç±»åˆ«æ•°é‡ (5)
    # y_score_for_auc æ˜¯è§„èŒƒåŒ–åçš„ P(1)åˆ°P(5)ï¼Œå…± 5 åˆ—ã€‚
    # binarized_true_labels ä¹Ÿæ˜¯ 5 åˆ—ï¼Œå¯¹åº”ç±»åˆ« 1åˆ°5ã€‚

    print("\n--- ç»˜åˆ¶ ROC æ›²çº¿ (OvR) ---")

    plt.figure(figsize=(10, 8))

    for i, class_id in enumerate(evaluated_class_ids):
        # binarized_true_labels[:, i] æ˜¯ç±»åˆ« class_id çš„äºŒå…ƒçœŸå®æ ‡ç­¾
        # y_score_for_auc[:, i] æ˜¯ç±»åˆ« class_id çš„é¢„æµ‹æ¦‚ç‡

        # è®¡ç®—å½“å‰ç±»åˆ«çš„ FPR, TPR
        fpr[class_id], tpr[class_id], _ = roc_curve(
            binarized_true_labels[:, i],
            y_score_for_auc[:, i]  # ä½¿ç”¨è§„èŒƒåŒ–åçš„æ¦‚ç‡
        )

        # è®¡ç®—å½“å‰ç±»åˆ«çš„ AUC (ä¸å®å¹³å‡ AUC å†…éƒ¨çš„å•ç±»åˆ« AUC ç›¸åŒ)
        roc_auc_single = auc(fpr[class_id], tpr[class_id])

        # ç»˜åˆ¶æ›²çº¿
        plt.plot(fpr[class_id], tpr[class_id],
                 label=f'{class_names[class_id]} (AUC = {roc_auc_single:.4f})')

    # ç»˜åˆ¶å¯¹è§’çº¿ (éšæœºåˆ†ç±»å™¨)
    plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.50)')

    # æ·»åŠ å®å¹³å‡ AUC æ–‡æœ¬ï¼ˆå¦‚æœè®¡ç®—æˆåŠŸï¼‰
    try:
        plt.text(0.8, 0.2, f'Macro-AUC: {auc_score_macro:.4f}',
                 fontsize=12, bbox=dict(facecolor='white', alpha=0.5))
    except NameError:
        pass  # å¦‚æœauc_score_macroæ²¡æœ‰è®¡ç®—æˆåŠŸï¼Œåˆ™è·³è¿‡

    # è®¾ç½®å›¾è¡¨æ ‡é¢˜å’Œè½´æ ‡ç­¾
    plt.xlabel('False Positive Rate (FPR)')
    plt.ylabel('True Positive Rate (TPR)')
    plt.title('Receiver Operating Characteristic (ROC) Curves (One-vs-Rest)')
    plt.legend(loc='lower right')
    plt.grid(True)

    # ä¿å­˜å›¾åƒ
    roc_save_path = '/home/idao/Zyf/models/oc_subtype_classficter/multi_view/roc_curves.png'
    plt.savefig(roc_save_path)
    print(f"ROC æ›²çº¿å·²ä¿å­˜åˆ°: {roc_save_path}")

    plt.show()